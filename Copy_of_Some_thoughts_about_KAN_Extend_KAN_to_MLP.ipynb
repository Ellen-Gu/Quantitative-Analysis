{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1x6_kzG0D20X2l2pghpzpsWDRSvm9Xfb6",
      "authorship_tag": "ABX9TyOh+8gcz1gDq7+sxyDtgBiG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ellen-Gu/Quantitative-Analysis/blob/main/Copy_of_Some_thoughts_about_KAN_Extend_KAN_to_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some thoughts about KAN: Extend KAN idea to MLP\n",
        "\n",
        "Recently there are a lot of articles about KAN in my news feedings. Today I went to the git of KAN to have a deep look. It is a very nice thought about the fundimental architecture of NN.\n",
        "\n",
        "KAN is much nice than MLP in Interpretability and accuracy. The difficulty of this novel thinking is in its computing speed. It is very slow in training and inference as some articles say. When scaling up this may be even difficult.\n",
        "\n",
        "However, if KAN is able to scale up with reasonable training and inference speed, as some asked, how about the investment of current LLM?\n",
        "\n",
        "Acturally I think current MLP may achieve what KAN wants to do, with cetain extra layers inserted. If going with spline, it will slow down the speed. In my tests, I see that both KAN and MLP has some difficulites in extending the supporting area. That may hurt the generalizations out of the scope of current data, but for the place where we have data, proper adjustments can make MLP suit for mimicing patterns.\n",
        "\n",
        "So, for the consideration of speed, we may not scale up that much with spline at this moment. But for MLP, taking in the activation part as a new layer (say, with indicator functions, or a classify layer) to get the benifit of combining different activation function to each of the neuron may boost the performance of current MLP architecture.\n",
        "\n",
        "The test I did is to use talyor expansion (to 7th polynomial items) as the basis in deep MLP, and repeatly changing the activation functions. Based on the results I think that to some extent, deep MLP can catch the pattern. It can not catch extreme cases, but for usual functions it can catch at lease one full period. In some cases I can get many periods, but for some special functions I can only get one full period, with other area generally the averages of the trends and all spikes out of the period are lost. Hence I think KAN faces similar issue, since it has to rely on spline to solve this period issue. However, though spline has nice outcomes, its speed is of problem. In current LLM we seem not encouter such kind of periodic issue. It may be because we have enough data, or because we naturally keep shifting the intaking windows.\n",
        "\n",
        "So the thinking here is that, if we model like KAN to make the activation functions specfic to each of the neuron instead of to all ones, then this will solve both the window shifting issue by layers and at the same time, get the benifit of customized activation functions. I think that inserting one clasify layer before the summation to pick the \"right\" activation function for each specific neuron will do the work. this definitly will slow down the training speed at the begining, but convergence speed may still be better than MLP. If someone can give this idea a test to see how it goes, it will be great."
      ],
      "metadata": {
        "id": "BPWGnydTqdBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzqjYcZFGXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I rethinking of this, maybe we just need to add one ore outputs for each neuron on some layers. Currently LLM's nueron are specifically assigned, they are to some sense \"equal\" when get weights updateing. If a specific nueron can take the task of identify proper activation functions, or add one more output to neron to let it handle the activation function selection, that could also do the work similar to KAN."
      ],
      "metadata": {
        "id": "KUmx8EYeELnv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpoeILp5EKXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}