{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlZfvqefgCaMHuJfuhaABu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ellen-Gu/Quantitative-Analysis/blob/main/Some_thoughts_about_KAN_Extend_KAN_to_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recently someone pushs a lot of articles about KAN to my news feedings.\n",
        "Finally I went to the git of KAN today to have a deep look.  It is a very nice thought about the fundimental architecture of NN.\n",
        "\n",
        "KAN is much nice than MLP in Interpretability and accuracy. The difficulty of this novel thinking is in its computing speed.\n",
        "It is very slow in training and inference as some articles say. When scale up this may be even difficult.\n",
        "\n",
        "However, if KAN is able to scale up with reasonable training and inference speed, as some asked, how about the investment of current LLM?\n",
        "\n",
        "Acturally I think current MLP may achieve what KAN wants to do, with cetain extra layers inserted. If going with spline, it will slow down the speed. In my tests, I see that both KAN and MLP has some difficulites in extending the supporting area. That may hurt the generalizations out of the scope of current data, but for the place where we have data, proper adjustments can make MLP suit for mimicing patterns.\n",
        "\n",
        "So, for the consideration of speed, we may not scale up that much with spline at this moment. But for MLP, taking in the activation part as a new layer (say, with indicator functions, or a classify layer) to get the benifit of combining different activation function to each of the neuron may boost the performance of current MLP architecture.\n",
        "\n",
        "The test I did is to use talyor expansion (to 7th polynomial items) as the basis in deep MLP, and repeatly changing the activation functions. From the results I think that to some extent, deep MLP can catch the pattern. It can not catch extreme cases, but for usual functions it can catch at lease one full period. In some cases I can get many periods, but for some special functions I can only get one full period, with other area generally the averages of the trends and all spikes out of the period are lost. Hence I think KAN faces similar issue, since it has to rely on spline to solve this period issue. However, though spline has nice outcomes, its speed is of problem. In current LLM we seem not encouter such kind of periodic issue. It may be because we have enough data, or because we naturally keep shifting the intaking windows.\n",
        "\n",
        "So the thinking here is that, if we model like KAN to make the activation functions specfic to each of the neuron instead of to all ones, then this will solve both the window shifting issue and at the same time, get the benifit of customized activation functions. I think that inserting one clasify layer before the summation to pick the \"right\" activation function for each specific neuron will do the work. this definitly will slow down the training speed at the begining, but convergence speed may still be better than MLP. I wish someone can give this idea a test to see how it goes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJezS-odf6ze"
      }
    }
  ]
}