{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcrwxRaVbZ9JMqVx8Kfs6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ellen-Gu/Quantitative-Analysis/blob/main/Integrating_Nested_Strategies_Gleaning_Deep_Learning_Wisdom_for_Enhanced_Tree_Based_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Nested Strategies: Gleaning Deep Learning Wisdom for Enhanced Tree-Based Predictions\n",
        "<br>\n",
        "\n",
        "Today I saw an article on medium, [Your Random Forest Model is Never the Best Random Forest Model You Can Build](https://medium.datadriveninvestor.com/your-random-forest-model-is-never-the-best-random-forest-model-you-can-build-e3d0b79080dc) (https://medium.datadriveninvestor.com/your-random-forest-model-is-never-the-best-random-forest-model-you-can-build-e3d0b79080dc). It is very interesting. Recently I also read some other articles about ResNet and RNN. Both have intriguing ideas, and I'd like to discuss them further.\n",
        "\n",
        "* **Insight into ResNet's Core Philosophy:**\n",
        "\n",
        "  The core idea behind ResNet is that searching within an enlarged but nested space can lead to better predictive power. This enlargement can be achieved by adding more layers to the neural network. To establish this \"nested\" space, the newly added layers should encompass the original solution space. This led to the introduction of residual blocks that form the foundation of ResNet, which won the ImageNet Large Scale Visual Recognition Challenge in 2015. The deailed operation involves fitting data with $f(x)-x$, then after the fitting add $x$ back. If viewing $x$ as an identity mapping function, then $f(x) -x $ represents the residual. With this way we still get $f(x)$, and the newly added residual layers reduced the traing errors. Later on, the DenseNet extends the idea in ResNet, as $f(x)-x$ is easily making us to think about taylor expansion expanded to its first item. In DenseNet, $f(x)$ is decompsed to more than just one item $x$, and hence the name of DenseNet.\n",
        "\n",
        "* **Random Forests and Efficient Predictions:**\n",
        "\n",
        "  The idea in the random forest article is very promising. It discussed the possibility of reducing prediction time and enhancing accuracy by selecting the best \"k\" random trees. This is especially enticing for production. The author presented a cumulative accuracy plot to juxtapose training accuracy against validation and test accuracy. The hands-on book mentions that a significant disparity between training and testing errors can indicate overfitting. The article's author also touched on this.\n",
        "\n",
        "* **Capturing Sparse Patterns:**\n",
        "\n",
        "  My thought is that while trees with lower accuracy might appear redundant for the training data, they could capture weak or sparse patterns. These patterns might incorporate features absent in the best \"k\" trees. To validate this hypothesis, one could use heavy sampling of \"sparse\" patterns, increase the amount of rare and \"unusual\" stratifications of data, introduce noise for stability tests, and simulate weak patterns in features not present in the \"k\" trees. This would help refine the initial idea.\n",
        "\n",
        "* **Classification Challenges and a Nested Approach:**\n",
        "\n",
        "  Typically, classification models use the final resultant model for predictions. From my experience, most classification scenarios have clear class labels. The challenge arises with borderline cases. I strongly agree with the author's idea of using the \"k\" trees to expedite predictions, as many instances should be easy to classify. Only the ambiguous ones might require the complete set of random forest trees for accurate prediction. Drawing inspiration from ResNet's success, I think that a combination of reduced prediction time and increased accuracy can be achieved through a nested algorithm.\n",
        "\n",
        "* **Suggested Model Improvement:**\n",
        "\n",
        "  My suggested improvement is as follows: Label the cases which can be classed by the k trees as 1 ( or by more trees which can be corrected classed cases), all others as 0. Then use this as the newly formed train data to do a simple logistic regression model. When in prediction, we first class the test data as 0 or 1 based on this simple logistic regression model (preferably with a higher threshold for probabilities). If it is 1, then use the k trees method, if 0 then use the complete set of random forest trees as usual. I suspect this approach might narrow the gap between training and testing accuracies.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B0ZmmCizVLRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vLIfsrSaHyYt"
      }
    }
  ]
}